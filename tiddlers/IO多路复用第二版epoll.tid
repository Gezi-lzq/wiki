created: 20230322074737902
creator: Gezi
modified: 20230323074011988
modifier: Gezi
tags: 网络演变
title: IO多路复用第二版epoll
type: text/vnd.tiddlywiki


!! 相对第一版的改进

select与poll中，从主动轮询转变为了被动通知，的确提升了性能。但是select()/poll()每次调用都需要拷贝管理的全量的fd到内核态，导致影响性能。

''改进思路：''

由第一版的<mark class="mark-orange">拷贝，模糊通知</mark>优化为=><mark class="mark-red">不拷贝，明确通知</mark>


1. epoll和poll的区别在于epoll维护了一个内核事件表存放所有文件描述符，而poll没有长久维护数据结构

2. poll每次调用时都会拷贝一遍所有文件描述符到内核态，而epoll不会

3.在内核中长久维护一个数据结构来存放文件描述符会影响内核效率

4.红黑树是一种高效的数据结构来存放文件描述符

5.epoll的操作EPOLL_CTL_ADD可以使用红黑树来实现插入操作

6.红黑树的插入、查找和删除操作效率都比较高

! 调用过程

[img[epoll函数调用过程.png]]

!! 执行过程

1. epoll_create创建eventpoll对象（红黑树，双链表）

2. 一棵红黑树，存储监听的所有文件描述符，并且通过epoll_ctl将文件描述符添加、删除到红黑树

3. 一个双链表，存储就绪的文件描述符列表，epoll_wait调用时，检测此链表中是否有数据，有的话直接返回；当有数据的时候，会把相应的文件描述符'置位'，但是epoll没有revent标志位，所以并不是真正的置位。这时候会把有数据的文件描述符放到队首。

4. 所有添加到eventpoll中的事件都与设备驱动程序建立回调关系；epoll会返回有数据的文件描述符的个数，根据返回的个数，读取前N个文件描述符即可

''事件回调通知机制''：

1. 当有网卡上有数据到达了，首先会放到DMA（内存中的一个buffer，网卡可以直接访问这个数据区域）中 

2. 网卡向cpu发起中断，让cpu先处理网卡的事 

3. 中断号在内存中会绑定一个回调，哪个socket中有数据，回调函数就把哪个socket放入就绪链表中

! epoll的工作模式（LT和ET触发）

!! 触发模式（LT模式）

LT模式也就是水平触发模式，是epoll的默认触发模式（select和poll只有这种模式） 

触发条件：

* 可读事件：接受缓冲区中的数据大小''高于低水位标记''，则会触发事件

* 可写事件：发送缓冲区中的剩余空间大小''大于低水位标记''，则会触发事件

** 低水位标记：一个基准值，默认是1 所以简单点说，水平触发模式就是''只要缓冲区中还有数据，就会一直触发事件''。当epoll检测到socket上事件就绪的时候，可以不立刻进行处理，或者只处理一部分。例如：由于只读了1K数据，缓冲区还剩下1K数据，在第二次调用`epoll_wait`时，`epoll_wait`仍然会立刻返回并通知socket读事件就绪，直到缓冲区上所有的数据都被处理完，`epoll_wait`才不会立刻返回。

!! 边缘触发模式（ET模式）

ET模式也就是边缘触发模式，如果我们将socket添加到epoll_event描述符的时候使用了EPOLLET标志，`epoll`就会进入ET工作模式。

触发条件

* 可读事件：(不关心接受缓冲区是否有数据)每当有新数据到来时，才会触发事件。(因此需要循环处理，直到缓冲区为空，一次性将所有数据取出)

* 可写事件：剩余空间从无到有的时候才会触发事件。


!! 区别

* LT模式的优点主要在于其简单且稳定，不容易出现问题，传统的select和poll都是使用这个模式。但是也有缺点，就是因为事件触发过多导致效率降低

* ET最大的优点就是减少了epoll的触发次数，但是这也带来了巨大的代价，就''是要求必须一次性将所有的数据处理完''，虽然效率得到了提高，但是代码的复杂程度大大的增加了。Nginx就是默认采用ET模式

* 还有一种场景适合ET模式使用，如果我们需要接受一条数据，但是这条数据因为某种问题导致其发送不完整，需要分批发送。所以此时的缓冲区中数据只有部分，如果此时将其取出，则会增加维护数据的开销。因此需要等待后续数据到达后将其补全，再一次性取出。但是如果此时使用的是LT模式，就会因为缓冲区不为空而一直触发事件，所以这种情况下使用ET会比较好。（拆包粘包问题）

[img[LT与ET的区别.png]]



